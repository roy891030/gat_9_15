# DMFM (Wei et al. 2022) å¯¦ä½œèªªæ˜

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

æœ¬å¯¦ä½œå®Œå…¨å°é½Š Wei et al. (2022) è«–æ–‡ "Deep Multi-Factor Model" çš„æ¶æ§‹ï¼ŒåŒ…å«ï¼š

1. **éšå±¤å¼é›™åœ–çµæ§‹**ï¼šç”¢æ¥­åœ– + å…¨å¸‚å ´åœ–
2. **ç”¢æ¥­ä¸­æ€§åŒ–**ï¼šC_I = C - H_I
3. **å…¨å¸‚å ´ä¸­æ€§åŒ–**ï¼šC_U = C_I - H_U
4. **éšå±¤å¼ç‰¹å¾µæ‹¼æ¥**ï¼š[C || C_I || C_U]
5. **Factor Attention æ¨¡çµ„**ï¼šè§£é‡‹æ·±åº¦å› å­ä¾†è‡ªå“ªäº›åŸå§‹ç‰¹å¾µ

---

## ğŸ†• èˆ‡åŸå§‹ä»£ç¢¼çš„å·®ç•°

### 1. è³‡æ–™é è™•ç†ï¼ˆ`build_artifacts.py`ï¼‰

**ä¿®æ”¹å‰ï¼š**
```python
Z = xsec_zscore(A)  # æˆªé¢æ¨™æº–åŒ–
df["yt"] = df.groupby(cm["date"])["fwd_ret_k"].transform(lambda s: s - np.nanmean(s))  # æ¨™ç±¤å»å‡å€¼
```

**ä¿®æ”¹å¾Œï¼š**
```python
# ä¸åšæˆªé¢æ¨™æº–åŒ–ï¼Œä¿ç•™åŸå§‹ç‰¹å¾µ
Ft[:,:,k] = np.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)
# æ¨™ç±¤ä¸å»å‡å€¼
df["yt"] = df["fwd_ret_k"]
```

**åŸå› ï¼š** è«–æ–‡ä½¿ç”¨ BatchNorm åœ¨æ¨¡å‹å…§éƒ¨åšæ¨™æº–åŒ–ï¼Œè€Œä¸æ˜¯é è™•ç†éšæ®µã€‚

---

### 2. æ¨¡å‹æ¶æ§‹ï¼ˆ`model_dmfm_wei2022.py`ï¼‰

#### **é—œéµå·®ç•° 1ï¼šGAT ä½¿ç”¨ `concat=False`**

**åŸå§‹ä»£ç¢¼ï¼ˆ`train_gat_fixed.py`ï¼‰ï¼š**
```python
self.gat_industry = GATConv(hid, hid, heads=heads)  # é è¨­ concat=True
# è¼¸å‡ºç¶­åº¦ï¼šhid * heads
```

**æ–°ä»£ç¢¼ï¼š**
```python
self.gat_industry = GATConv(hidden_dim, hidden_dim, heads=heads, concat=False)
# è¼¸å‡ºç¶­åº¦ï¼šhidden_dimï¼ˆå¹³å‡å¤šé ­ï¼‰
```

#### **é—œéµå·®ç•° 2ï¼šéšå±¤å¼ä¸­æ€§åŒ–**

**åŸå§‹ä»£ç¢¼ï¼š**
```python
C_t_expanded = C_t.repeat(1, self.gat_industry.heads)  # éœ€è¦æ“´å±•ç¶­åº¦
C_bar_I = C_t_expanded - H_I
```

**æ–°ä»£ç¢¼ï¼š**
```python
# ä¸éœ€è¦æ“´å±•ç¶­åº¦
C_I = C - H_I  # ç”¢æ¥­ä¸­æ€§åŒ–
C_U = C_I - H_U  # å…¨å¸‚å ´ä¸­æ€§åŒ–ï¼ˆæ³¨æ„ï¼šè¼¸å…¥æ˜¯ C_Iï¼‰
```

#### **é—œéµå·®ç•° 3ï¼šå…¨å¸‚å ´ GAT çš„è¼¸å…¥**

**åŸå§‹ä»£ç¢¼ï¼š** å…¨å¸‚å ´ GAT çš„è¼¸å…¥ä¸æ˜ç¢º

**æ–°ä»£ç¢¼ï¼š**
```python
H_I = self.gat_industry(C, industry_edge_index)
C_I = C - H_I
H_U = self.gat_universe(C_I, universe_edge_index)  # â† è¼¸å…¥æ˜¯ç”¢æ¥­ä¸­æ€§åŒ–å¾Œçš„ C_Iï¼
C_U = C_I - H_U
```

---

### 3. æå¤±å‡½æ•¸ï¼ˆ`train_dmfm_wei2022.py`ï¼‰

**è«–æ–‡å…¬å¼ 13ï¼š**
```
L = Î»_attn Â· d - b + Î»_IC Â· (1 - IC)
```

å…¶ä¸­ï¼š
- `d`ï¼šAttention estimate loss = ||f - f_hat||Â²
- `b`ï¼šFactor return (cross-sectional regression)
- `IC`ï¼šInformation Coefficient

**å¯¦ä½œï¼š**
```python
def compute_loss(deep_factor, f_hat, returns, lambda_attn, lambda_ic):
    d = torch.norm(deep_factor - f_hat, p=2)
    b = cross_sectional_regression(deep_factor, returns)
    ic = compute_ic(deep_factor, returns)
    ic_penalty = 1.0 - ic

    loss = lambda_attn * d + lambda_ic * ic_penalty - b
    return loss
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. å®‰è£ä¾è³´

```bash
pip install torch numpy pandas matplotlib seaborn scikit-learn tqdm torch-geometric
```

### 2. ä¸€éµåŸ·è¡Œå®Œæ•´æµç¨‹

```bash
bash run_dmfm_wei2022.sh
```

é€™å€‹è…³æœ¬æœƒè‡ªå‹•åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š
1. å»ºç«‹ Artifactsï¼ˆæ–°çš„é è™•ç†æ–¹å¼ï¼‰
2. è¨“ç·´ DMFM æ¨¡å‹
3. è¦–è¦ºåŒ– Factor Attention
4. åˆ†æéšå±¤å¼ç‰¹å¾µ
5. è©•ä¼°æ¨¡å‹æŒ‡æ¨™
6. æŠ•è³‡çµ„åˆå›æ¸¬

---

## ğŸ“‚ æª”æ¡ˆçµæ§‹

```
gat_9_15/
â”œâ”€â”€ build_artifacts.py           # ä¿®æ”¹ï¼šç§»é™¤æˆªé¢æ¨™æº–åŒ–
â”œâ”€â”€ model_dmfm_wei2022.py         # æ–°å¢ï¼šå®Œæ•´ DMFM æ¶æ§‹
â”œâ”€â”€ train_dmfm_wei2022.py         # æ–°å¢ï¼šè«–æ–‡æå¤±å‡½æ•¸
â”œâ”€â”€ visualize_factor_attention.py # æ–°å¢ï¼šFactor Attention è¦–è¦ºåŒ–
â”œâ”€â”€ analyze_contexts.py           # æ–°å¢ï¼šéšå±¤å¼ç‰¹å¾µåˆ†æ
â”œâ”€â”€ run_dmfm_wei2022.sh           # æ–°å¢ï¼šå®Œæ•´åŸ·è¡Œæµç¨‹
â””â”€â”€ README_DMFM_Wei2022.md        # æ–°å¢ï¼šèªªæ˜æ–‡ä»¶
```

---

## ğŸ” åˆ†æ­¥åŸ·è¡Œ

å¦‚æœä½ æƒ³æ‰‹å‹•åŸ·è¡Œå„å€‹æ­¥é©Ÿï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹é †åºï¼š

### Step 1: å»ºç«‹ Artifacts

```bash
python build_artifacts.py \
  --prices unique_2019q3to2025q3.csv \
  --industry_csv unique_2019q3to2025q3.csv \
  --artifact_dir gat_artifacts_wei2022 \
  --start_date 2019-09-16 \
  --end_date 2025-09-12 \
  --horizon 5
```

**è¼¸å‡ºï¼š**
- `gat_artifacts_wei2022/Ft_tensor.pt` - ç‰¹å¾µå¼µé‡ï¼ˆæœªæ¨™æº–åŒ–ï¼‰
- `gat_artifacts_wei2022/yt_tensor.pt` - æ¨™ç±¤å¼µé‡ï¼ˆæœªå»å‡å€¼ï¼‰
- `gat_artifacts_wei2022/industry_edge_index.pt` - ç”¢æ¥­åœ–
- `gat_artifacts_wei2022/universe_edge_index.pt` - å…¨å¸‚å ´åœ–

---

### Step 2: è¨“ç·´æ¨¡å‹

```bash
python train_dmfm_wei2022.py \
  --artifact_dir gat_artifacts_wei2022 \
  --epochs 200 \
  --lr 1e-3 \
  --device cuda \
  --hidden_dim 64 \
  --heads 2 \
  --dropout 0.1 \
  --lambda_attn 0.1 \
  --lambda_ic 1.0
```

**è¼¸å‡ºï¼š**
- `gat_artifacts_wei2022/dmfm_wei2022_best.pt` - æœ€ä½³æ¨¡å‹
- `gat_artifacts_wei2022/train_log_wei2022.txt` - è¨“ç·´æ—¥èªŒ

**è¨“ç·´æŒ‡æ¨™ï¼š**
- Train Loss
- Train IC
- Test IC
- Test ICIR
- Test Factor Return

---

### Step 3: è¦–è¦ºåŒ– Factor Attention

```bash
python visualize_factor_attention.py \
  --artifact_dir gat_artifacts_wei2022 \
  --output_dir plots_attention_wei2022 \
  --top_k 15
```

**è¼¸å‡ºåœ–è¡¨ï¼š**
- `factor_attention_top_features.png` - Top K ç‰¹å¾µé‡è¦æ€§
- `factor_attention_all_features.png` - æ‰€æœ‰ç‰¹å¾µæ’åº
- `factor_attention_timeseries.png` - Top 5 ç‰¹å¾µçš„æ™‚é–“åºåˆ—
- `factor_attention_heatmap.png` - Top 20 ç‰¹å¾µç†±åŠ›åœ–
- `factor_attention_pie.png` - ç‰¹å¾µé‡è¦æ€§åˆ†å¸ƒé¤…åœ–
- `factor_attention_summary.txt` - çµ±è¨ˆæ‘˜è¦

---

### Step 4: åˆ†æéšå±¤å¼ç‰¹å¾µ

```bash
python analyze_contexts.py \
  --artifact_dir gat_artifacts_wei2022 \
  --output_dir plots_contexts_wei2022 \
  --sample_days 20
```

**è¼¸å‡ºåœ–è¡¨ï¼š**
- `context_distributions.png` - ä¸‰ç¨®ç‰¹å¾µçš„åˆ†å¸ƒæ¯”è¼ƒ
- `variance_reduction.png` - è®Šç•°æ•¸é™ä½æ•ˆæœ
- `variance_reduction_percentage.png` - è®Šç•°æ•¸é™ä½ç™¾åˆ†æ¯”
- `influence_magnitude.png` - å½±éŸ¿åŠ›å¤§å°åˆ†å¸ƒ
- `context_pca_projection.png` - 2D PCA æŠ•å½±
- `influence_comparison.png` - ç”¢æ¥­ vs å…¨å¸‚å ´å½±éŸ¿åŠ›
- `context_analysis_summary.txt` - çµ±è¨ˆæ‘˜è¦

---

## ğŸ“Š é—œéµæ¦‚å¿µè§£é‡‹

### 1. ç”¢æ¥­ä¸­æ€§åŒ– (Industry Neutralization)

**ç›®çš„ï¼š** ç§»é™¤ç”¢æ¥­å…±åŒå½±éŸ¿ï¼Œä¿ç•™å€‹è‚¡è¶…é¡è¡¨ç¾

**å…¬å¼ï¼š**
```
H_I = GAT(C, Industry_Graph)  # å­¸ç¿’ç”¢æ¥­å…§çš„å…±åŒå½±éŸ¿
C_I = C - H_I                  # ç§»é™¤ç”¢æ¥­å½±éŸ¿
```

**ç¯„ä¾‹ï¼š**
- æ•´å€‹åŠå°é«”ç”¢æ¥­éƒ½åœ¨æ¼² +5%
- å°ç©é›»æ¼² +8%
- ç”¢æ¥­ä¸­æ€§åŒ–å¾Œï¼Œå°ç©é›»çš„ç‰¹å¾µä¿ç•™ +3%ï¼ˆè¶…é¡éƒ¨åˆ†ï¼‰

---

### 2. å…¨å¸‚å ´ä¸­æ€§åŒ– (Universe Neutralization)

**ç›®çš„ï¼š** ç§»é™¤å…¨å¸‚å ´å…±åŒå½±éŸ¿ï¼Œä¿ç•™ç´”å€‹è‚¡æ•ˆæ‡‰

**å…¬å¼ï¼š**
```
H_U = GAT(C_I, Universe_Graph)  # â† æ³¨æ„ï¼šè¼¸å…¥æ˜¯ C_I
C_U = C_I - H_U                  # ç§»é™¤å…¨å¸‚å ´å½±éŸ¿
```

**ç¯„ä¾‹ï¼š**
- å…¨å¸‚å ´éƒ½åœ¨æ¼² +2%ï¼ˆå¤šé ­å¸‚å ´ï¼‰
- å°ç©é›»ï¼ˆç”¢æ¥­ä¸­æ€§åŒ–å¾Œï¼‰æ¼² +3%
- å…¨å¸‚å ´ä¸­æ€§åŒ–å¾Œï¼Œå°ç©é›»çš„ç‰¹å¾µä¿ç•™ +1%ï¼ˆç´”å€‹è‚¡æ•ˆæ‡‰ï¼‰

---

### 3. éšå±¤å¼ç‰¹å¾µæ‹¼æ¥

**å…¬å¼ï¼š**
```
[C || C_I || C_U]
```

**ä¸‰ç¨®ç‰¹å¾µçš„æ„ç¾©ï¼š**
- **C**ï¼šåŸå§‹ç·¨ç¢¼ç‰¹å¾µï¼ˆåŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼‰
- **C_I**ï¼šç”¢æ¥­ä¸­æ€§åŒ–ç‰¹å¾µï¼ˆç§»é™¤ç”¢æ¥­æ•ˆæ‡‰ï¼‰
- **C_U**ï¼šå…¨å¸‚å ´ä¸­æ€§åŒ–ç‰¹å¾µï¼ˆç§»é™¤ç”¢æ¥­ + å¸‚å ´æ•ˆæ‡‰ï¼‰

**ç‚ºä»€éº¼æ‹¼æ¥ï¼Ÿ**
- è®“æ¨¡å‹åŒæ™‚å­¸ç¿’ä¸åŒå±¤æ¬¡çš„ä¿¡æ¯
- å…¨å±€ä¿¡æ¯ï¼ˆCï¼‰+ ç”¢æ¥­å…§ä¿¡æ¯ï¼ˆC_Iï¼‰+ ç´”å€‹è‚¡ä¿¡æ¯ï¼ˆC_Uï¼‰

---

### 4. Factor Attention æ¨¡çµ„

**ç›®çš„ï¼š** è§£é‡‹æ·±åº¦å› å­ä¾†è‡ªå“ªäº›åŸå§‹ç‰¹å¾µ

**å…¬å¼ï¼š**
```
U = LeakyReLU(W Â· F)  # å­¸ç¿’æ³¨æ„åŠ›é‚è¼¯
A = Softmax(U)        # æ­¸ä¸€åŒ–ç‚ºæ¬Šé‡
f_hat = F^T Â· A       # æ³¨æ„åŠ›ä¼°è¨ˆçš„å› å­
```

**æå¤±å‡½æ•¸ï¼š**
```
d = ||f - f_hat||Â²  # æœ€å°åŒ–æ·±åº¦å› å­èˆ‡æ³¨æ„åŠ›ä¼°è¨ˆçš„å·®ç•°
```

**æ‡‰ç”¨ï¼š**
- æŸ¥çœ‹å“ªäº›æŠ€è¡“æŒ‡æ¨™æœ€é‡è¦ï¼ˆå¦‚ RSIã€MACDï¼‰
- äº†è§£æ¨¡å‹çš„æ±ºç­–ä¾æ“š
- æé«˜æ¨¡å‹çš„å¯è§£é‡‹æ€§

---

## ğŸ“ˆ é æœŸçµæœ

### è¨“ç·´æŒ‡æ¨™

| æŒ‡æ¨™ | é æœŸç¯„åœ | èªªæ˜ |
|------|----------|------|
| Test IC | 0.03 - 0.08 | Information Coefficient |
| Test ICIR | 0.5 - 1.5 | IC ç©©å®šæ€§ |
| Factor Return | > 0 | å› å­ç´¯ç©æ”¶ç›Š |

### Factor Attention

**é æœŸç™¼ç¾ï¼š**
- å‹•é‡é¡ç‰¹å¾µï¼ˆret_10, ret_20ï¼‰é€šå¸¸æ¬Šé‡è¼ƒé«˜
- æŠ€è¡“æŒ‡æ¨™ï¼ˆRSI, MACDï¼‰ä¹Ÿæœ‰é¡¯è‘—è²¢ç»
- Top 10 ç‰¹å¾µé€šå¸¸ä½”ç¸½æ¬Šé‡çš„ 40-60%

### éšå±¤å¼ç‰¹å¾µåˆ†æ

**é æœŸæ•ˆæœï¼š**
- ç”¢æ¥­ä¸­æ€§åŒ–ï¼šè®Šç•°æ•¸é™ä½ 10-30%
- å…¨å¸‚å ´ä¸­æ€§åŒ–ï¼šè®Šç•°æ•¸å†é™ä½ 5-15%
- ç¸½é«”è®Šç•°æ•¸é™ä½ï¼š15-40%

---

## ğŸ› æ•…éšœæ’é™¤

### å•é¡Œ 1: CUDA Out of Memory

**è§£æ±ºæ–¹æ¡ˆ 1ï¼š** ä½¿ç”¨è¼ƒå°çš„ hidden_dim
```bash
python train_dmfm_wei2022.py --hidden_dim 32 --device cuda
```

**è§£æ±ºæ–¹æ¡ˆ 2ï¼š** ä½¿ç”¨ CPU
```bash
python train_dmfm_wei2022.py --device cpu
```

**è§£æ±ºæ–¹æ¡ˆ 3ï¼š** ä½¿ç”¨è¼•é‡ç‰ˆæ¨¡å‹ï¼ˆéœ€ä¿®æ”¹ä»£ç¢¼ï¼‰
```python
from model_dmfm_wei2022 import DMFM_Lite
model = DMFM_Lite(num_features=F, hidden_dim=32, heads=2)
```

---

### å•é¡Œ 2: æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ

**éŒ¯èª¤è¨Šæ¯ï¼š**
```
éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ gat_artifacts_wei2022/dmfm_wei2022_best.pt
```

**è§£æ±ºæ–¹æ¡ˆï¼š** å…ˆè¨“ç·´æ¨¡å‹
```bash
python train_dmfm_wei2022.py --artifact_dir gat_artifacts_wei2022
```

---

### å•é¡Œ 3: åœ–è¡¨ä¸­æ–‡äº‚ç¢¼

**è§£æ±ºæ–¹æ¡ˆï¼š** å®‰è£ä¸­æ–‡å­—é«”
```bash
# macOS
brew install font-source-han-sans

# Ubuntu
sudo apt-get install fonts-noto-cjk
```

æˆ–ä¿®æ”¹ `visualize_factor_attention.py` å’Œ `analyze_contexts.py`ï¼š
```python
plt.rcParams['font.sans-serif'] = ['Arial']  # ä½¿ç”¨è‹±æ–‡å­—é«”
```

---

## ğŸ“š åƒè€ƒæ–‡ç»

Wei, L., Li, B., & Chen, Y. (2022). Deep Multi-Factor Model for Stock Prediction.
*Journal of Machine Learning Research*.

---

## ğŸ‘¤ ä½œè€…

**Lo Yi (ç¾…é ¤)**
National Yang Ming Chiao Tung University
Graduate Institute of Information Management & Finance
E-mail: roy60404@gmail.com

---

## ğŸ“ ç‰ˆæœ¬è¨˜éŒ„

- **v1.0.0** (2025-01-XX): åˆå§‹ç‰ˆæœ¬ï¼Œå®Œå…¨å°é½Š Wei et al. (2022) è«–æ–‡
