# DMFM Wei et al. (2022) å¯¦ä½œ - è®Šæ›´æ¸…å–®

## ğŸ“‹ è®Šæ›´æ‘˜è¦

æœ¬æ¬¡æ›´æ–°å®Œå…¨å°é½Š Wei et al. (2022) è«–æ–‡æ¶æ§‹ï¼Œæ–°å¢äº†ä»¥ä¸‹æª”æ¡ˆå’Œä¿®æ”¹ï¼š

---

## ğŸ†• æ–°å¢æª”æ¡ˆ

### 1. `model_dmfm_wei2022.py`
**å®Œæ•´çš„ DMFM æ¨¡å‹å¯¦ä½œ**

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
- Stock Context Encoder (BatchNorm + MLP)
- Industry Neutralization (ç”¢æ¥­ä¸­æ€§åŒ–)
- Universe Neutralization (å…¨å¸‚å ´ä¸­æ€§åŒ–)
- Hierarchical Feature Concatenation (éšå±¤å¼ç‰¹å¾µæ‹¼æ¥)
- Deep Factor Learning (æ·±åº¦å› å­å­¸ç¿’)
- Factor Attention Module (å› å­æ³¨æ„åŠ›æ¨¡çµ„)

**é—œéµç‰¹æ€§ï¼š**
- GAT ä½¿ç”¨ `concat=False` (å¹³å‡å¤šé ­è¼¸å‡º)
- å…¨å¸‚å ´ GAT è¼¸å…¥æ˜¯ç”¢æ¥­ä¸­æ€§åŒ–å¾Œçš„ `C_I`
- ä¸‰ç¨®ç‰¹å¾µæ‹¼æ¥ï¼š`[C || C_I || C_U]`

---

### 2. `train_dmfm_wei2022.py`
**è¨“ç·´è…³æœ¬ï¼Œä½¿ç”¨è«–æ–‡çš„æå¤±å‡½æ•¸**

**æå¤±å‡½æ•¸ï¼ˆè«–æ–‡å…¬å¼ 13ï¼‰ï¼š**
```
L = Î»_attn Â· d + Î»_IC Â· (1 - IC) - b
```

å…¶ä¸­ï¼š
- `d`: Attention estimate loss = ||f - f_hat||Â²
- `b`: Factor return (cross-sectional regression)
- `IC`: Information Coefficient

**è¨“ç·´æŒ‡æ¨™ï¼š**
- Train/Test IC
- Train/Test ICIR
- Cumulative Factor Return
- Attention Distance

---

### 3. `visualize_factor_attention.py`
**Factor Attention è¦–è¦ºåŒ–å·¥å…·**

**ç”Ÿæˆçš„åœ–è¡¨ï¼š**
1. `factor_attention_top_features.png` - Top K ç‰¹å¾µé‡è¦æ€§
2. `factor_attention_all_features.png` - æ‰€æœ‰ç‰¹å¾µæ’åº
3. `factor_attention_timeseries.png` - Top 5 ç‰¹å¾µæ™‚é–“åºåˆ—
4. `factor_attention_heatmap.png` - Top 20 ç‰¹å¾µç†±åŠ›åœ–
5. `factor_attention_pie.png` - ç‰¹å¾µé‡è¦æ€§åˆ†å¸ƒé¤…åœ–
6. `factor_attention_summary.txt` - çµ±è¨ˆæ‘˜è¦

**åˆ†æå…§å®¹ï¼š**
- å“ªäº›åŸå§‹ç‰¹å¾µå°æ·±åº¦å› å­æœ€é‡è¦
- ç‰¹å¾µæ¬Šé‡çš„æ™‚é–“åºåˆ—è®ŠåŒ–
- Top 10/20 ç‰¹å¾µçš„ç¸½æ¬Šé‡ä½”æ¯”

---

### 4. `analyze_contexts.py`
**éšå±¤å¼ç‰¹å¾µåˆ†æå·¥å…·**

**ç”Ÿæˆçš„åœ–è¡¨ï¼š**
1. `context_distributions.png` - ä¸‰ç¨®ç‰¹å¾µåˆ†å¸ƒæ¯”è¼ƒ
2. `variance_reduction.png` - è®Šç•°æ•¸é™ä½æ•ˆæœ
3. `variance_reduction_percentage.png` - è®Šç•°æ•¸é™ä½ç™¾åˆ†æ¯”
4. `influence_magnitude.png` - å½±éŸ¿åŠ›å¤§å°åˆ†å¸ƒ
5. `context_pca_projection.png` - 2D PCA æŠ•å½±
6. `influence_comparison.png` - ç”¢æ¥­ vs å…¨å¸‚å ´å½±éŸ¿åŠ›
7. `context_analysis_summary.txt` - çµ±è¨ˆæ‘˜è¦

**åˆ†æå…§å®¹ï¼š**
- ç”¢æ¥­ä¸­æ€§åŒ–æ•ˆæœï¼ˆC â†’ C_Iï¼‰
- å…¨å¸‚å ´ä¸­æ€§åŒ–æ•ˆæœï¼ˆC_I â†’ C_Uï¼‰
- ç”¢æ¥­å½±éŸ¿ (H_I) vs å…¨å¸‚å ´å½±éŸ¿ (H_U)
- è®Šç•°æ•¸é™ä½ç™¾åˆ†æ¯”

---

### 5. `run_dmfm_wei2022.sh`
**å®Œæ•´åŸ·è¡Œæµç¨‹è…³æœ¬**

**åŸ·è¡Œæ­¥é©Ÿï¼š**
1. å»ºç«‹ Artifactsï¼ˆæ–°çš„é è™•ç†æ–¹å¼ï¼‰
2. è¨“ç·´ DMFM æ¨¡å‹
3. è¦–è¦ºåŒ– Factor Attention
4. åˆ†æéšå±¤å¼ç‰¹å¾µ
5. è©•ä¼°æ¨¡å‹æŒ‡æ¨™
6. æŠ•è³‡çµ„åˆå›æ¸¬

**ä½¿ç”¨æ–¹å¼ï¼š**
```bash
chmod +x run_dmfm_wei2022.sh
bash run_dmfm_wei2022.sh
```

---

### 6. `README_DMFM_Wei2022.md`
**å®Œæ•´çš„èªªæ˜æ–‡ä»¶**

**åŒ…å«å…§å®¹ï¼š**
- èˆ‡åŸå§‹ä»£ç¢¼çš„å·®ç•°èªªæ˜
- å¿«é€Ÿé–‹å§‹æŒ‡å—
- åˆ†æ­¥åŸ·è¡Œæ•™å­¸
- é—œéµæ¦‚å¿µè§£é‡‹ï¼ˆç”¢æ¥­ä¸­æ€§åŒ–ã€å…¨å¸‚å ´ä¸­æ€§åŒ–ã€Factor Attentionï¼‰
- é æœŸçµæœ
- æ•…éšœæ’é™¤

---

## ğŸ”§ ä¿®æ”¹æª”æ¡ˆ

### 1. `build_artifacts.py`

#### ä¿®æ”¹ 1: ç§»é™¤æˆªé¢æ¨™æº–åŒ–ï¼ˆLine 349-351ï¼‰

**ä¿®æ”¹å‰ï¼š**
```python
Z = xsec_zscore(A)  # æ¯æ—¥æˆªé¢ z-score
Ft[:,:,k] = np.nan_to_num(Z, nan=0.0, posinf=0.0, neginf=0.0)
```

**ä¿®æ”¹å¾Œï¼š**
```python
# â­ ä¿®æ”¹ï¼šä¸åšæˆªé¢æ¨™æº–åŒ–ï¼Œä¿ç•™åŸå§‹ç‰¹å¾µå€¼
# DMFM æ¨¡å‹æœƒä½¿ç”¨ BatchNorm åšæ¨™æº–åŒ–
Ft[:,:,k] = np.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)
```

**åŸå› ï¼š** è«–æ–‡ä½¿ç”¨ BatchNorm åœ¨æ¨¡å‹å…§éƒ¨åšæ¨™æº–åŒ–

---

#### ä¿®æ”¹ 2: ç§»é™¤æ¨™ç±¤å»å‡å€¼ï¼ˆLine 314-315ï¼‰

**ä¿®æ”¹å‰ï¼š**
```python
# Labelï¼šæœªä¾† k æ—¥å ±é…¬ â†’ æ¯æ—¥æˆªé¢å»å‡å€¼
df["yt"] = df.groupby(cm["date"])["fwd_ret_k"].transform(lambda s: s - np.nanmean(s.values))
```

**ä¿®æ”¹å¾Œï¼š**
```python
# Labelï¼šæœªä¾† k æ—¥å ±é…¬ï¼ˆä¿ç•™åŸå§‹å€¼ï¼Œä¸åšæˆªé¢å»å‡å€¼ï¼‰
# â­ ä¿®æ”¹ï¼šä¸åšæˆªé¢å»å‡å€¼ï¼Œå› ç‚º DMFM æ¨¡å‹æœƒåœ¨è¨“ç·´æ™‚è™•ç†
df["yt"] = df["fwd_ret_k"]
```

**åŸå› ï¼š** ä¿ç•™åŸå§‹å ±é…¬ç‡ï¼Œè®“æ¨¡å‹å­¸ç¿’çµ•å°æ”¶ç›Š

---

## ğŸ“Š æ¶æ§‹å°æ¯”

### åŸå§‹ DMFM (train_gat_fixed.py:176-279)

```python
class DMFM(nn.Module):
    def __init__(self, in_dim, hid=64, heads=2, ...):
        self.encoder = nn.Sequential(...)
        self.gat_industry = GATConv(hid, hid, heads=heads)  # concat=True (é è¨­)
        self.gat_universe = GATConv(hid*heads, hid, heads=heads)
        self.factor_head = nn.Sequential(...)

    def forward(self, x_raw, edge_industry, edge_universe):
        C_t = self.encoder(x_raw)  # [N, hid]
        H_I = self.gat_industry(C_t, edge_industry)  # [N, hid*heads]
        C_t_expanded = C_t.repeat(1, self.gat_industry.heads)  # éœ€è¦æ“´å±•
        C_bar_I = C_t_expanded - H_I
        H_U = self.gat_universe(C_bar_I, edge_universe)
        C_bar_U = C_bar_I - H_U
        hierarchical_features = torch.cat([C_t_expanded, C_bar_I, C_bar_U], dim=-1)
        deep_factor = self.factor_head(hierarchical_features).squeeze(-1)
        # ... (Factor Attention)
```

**å•é¡Œï¼š**
- GAT ä½¿ç”¨ `concat=True`ï¼Œå°è‡´ç¶­åº¦è®ŠåŒ–è¤‡é›œ
- éœ€è¦æ‰‹å‹•æ“´å±• `C_t` çš„ç¶­åº¦
- éšå±¤å¼ä¸­æ€§åŒ–ä¸å¤ æ¸…æ™°

---

### æ–°ç‰ˆ DMFM_Wei2022 (model_dmfm_wei2022.py)

```python
class DMFM_Wei2022(nn.Module):
    def __init__(self, num_features, hidden_dim=64, heads=2, ...):
        self.batch_norm = nn.BatchNorm1d(num_features)
        self.encoder = nn.Sequential(...)
        self.gat_industry = GATConv(hidden_dim, hidden_dim, heads=heads, concat=False)
        self.gat_universe = GATConv(hidden_dim, hidden_dim, heads=heads, concat=False)
        self.factor_decoder = nn.Sequential(...)
        self.factor_attention = nn.Linear(num_features, num_features)

    def forward(self, x, industry_edge_index, universe_edge_index):
        # Step 1: BatchNorm + Encoding
        x_norm = self.batch_norm(x)  # [N, F]
        C = self.encoder(x_norm)  # [N, hidden_dim]

        # Step 2: Industry Neutralization
        H_I = self.gat_industry(C, industry_edge_index)  # [N, hidden_dim]
        H_I = F.elu(H_I)
        C_I = C - H_I  # ç”¢æ¥­ä¸­æ€§åŒ–

        # Step 3: Universe Neutralization
        H_U = self.gat_universe(C_I, universe_edge_index)  # [N, hidden_dim]
        H_U = F.elu(H_U)
        C_U = C_I - H_U  # å…¨å¸‚å ´ä¸­æ€§åŒ–

        # Step 4: Hierarchical Feature Concatenation
        hierarchical_features = torch.cat([C, C_I, C_U], dim=-1)  # [N, 3*hidden_dim]

        # Step 5: Deep Factor
        deep_factor = self.factor_decoder(hierarchical_features)  # [N, 1]

        # Step 6: Factor Attention
        U = F.leaky_relu(self.factor_attention(x), negative_slope=0.2)
        attn_weights = F.softmax(U, dim=-1)

        contexts = {'C': C, 'C_I': C_I, 'C_U': C_U, 'H_I': H_I, 'H_U': H_U}
        return deep_factor, attn_weights, contexts
```

**å„ªé»ï¼š**
- GAT ä½¿ç”¨ `concat=False`ï¼Œç¶­åº¦çµ±ä¸€ç‚º `hidden_dim`
- ä¸éœ€è¦æ‰‹å‹•æ“´å±•ç¶­åº¦
- éšå±¤å¼ä¸­æ€§åŒ–æ¸…æ™°æ˜ç­ï¼š`C â†’ C_I â†’ C_U`
- å®Œå…¨å°é½Šè«–æ–‡æ¶æ§‹

---

## ğŸ”‘ é—œéµæ”¹é€²

### 1. ç”¢æ¥­ä¸­æ€§åŒ–ï¼ˆIndustry Neutralizationï¼‰

**å…¬å¼ï¼š**
```
H_I = GAT(C, Industry_Graph)  # å­¸ç¿’ç”¢æ¥­å…§å½±éŸ¿
C_I = C - H_I                  # ç§»é™¤ç”¢æ¥­å½±éŸ¿
```

**æ„ç¾©ï¼š** ç§»é™¤ã€Œç”¢æ¥­æ•ˆæ‡‰ã€ï¼Œä¿ç•™ã€Œå€‹è‚¡è¶…é¡è¡¨ç¾ã€

---

### 2. å…¨å¸‚å ´ä¸­æ€§åŒ–ï¼ˆUniverse Neutralizationï¼‰

**å…¬å¼ï¼š**
```
H_U = GAT(C_I, Universe_Graph)  # â­ è¼¸å…¥æ˜¯ C_Iï¼Œä¸æ˜¯ Cï¼
C_U = C_I - H_U                  # ç§»é™¤å…¨å¸‚å ´å½±éŸ¿
```

**æ„ç¾©ï¼š** ç§»é™¤ã€Œå…¨å¸‚å ´å…±åŒå½±éŸ¿ã€ï¼Œä¿ç•™ã€Œç´”å€‹è‚¡æ•ˆæ‡‰ã€

---

### 3. éšå±¤å¼ç‰¹å¾µæ‹¼æ¥ï¼ˆHierarchical Feature Concatenationï¼‰

**å…¬å¼ï¼š**
```
[C || C_I || C_U]
```

**æ„ç¾©ï¼š**
- **C**ï¼šåŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼ˆåŸå§‹ç·¨ç¢¼ç‰¹å¾µï¼‰
- **C_I**ï¼šç§»é™¤ç”¢æ¥­æ•ˆæ‡‰
- **C_U**ï¼šç§»é™¤ç”¢æ¥­ + å¸‚å ´æ•ˆæ‡‰

è®“æ¨¡å‹åŒæ™‚å­¸ç¿’ä¸åŒå±¤æ¬¡çš„ä¿¡æ¯ã€‚

---

### 4. Factor Attention æ¨¡çµ„

**å…¬å¼ï¼š**
```
U = LeakyReLU(W Â· F)  # å­¸ç¿’æ³¨æ„åŠ›é‚è¼¯
A = Softmax(U)        # æ­¸ä¸€åŒ–ç‚ºæ¬Šé‡
f_hat = F^T Â· A       # æ³¨æ„åŠ›ä¼°è¨ˆçš„å› å­
```

**æå¤±ï¼š**
```
d = ||f - f_hat||Â²  # æœ€å°åŒ–æ·±åº¦å› å­èˆ‡æ³¨æ„åŠ›ä¼°è¨ˆçš„å·®ç•°
```

**æ„ç¾©ï¼š** æé«˜æ¨¡å‹å¯è§£é‡‹æ€§ï¼Œäº†è§£å“ªäº›åŸå§‹ç‰¹å¾µæœ€é‡è¦ã€‚

---

## ğŸ“ˆ ä½¿ç”¨æµç¨‹

### å¿«é€Ÿé–‹å§‹

```bash
# ä¸€éµåŸ·è¡Œå®Œæ•´æµç¨‹
bash run_dmfm_wei2022.sh
```

### åˆ†æ­¥åŸ·è¡Œ

```bash
# Step 1: å»ºç«‹ Artifacts
python build_artifacts.py \
  --prices unique_2019q3to2025q3.csv \
  --industry_csv unique_2019q3to2025q3.csv \
  --artifact_dir gat_artifacts_wei2022 \
  --start_date 2019-09-16 \
  --end_date 2025-09-12 \
  --horizon 5

# Step 2: è¨“ç·´æ¨¡å‹
python train_dmfm_wei2022.py \
  --artifact_dir gat_artifacts_wei2022 \
  --epochs 200 \
  --lr 1e-3 \
  --device cuda

# Step 3: è¦–è¦ºåŒ– Factor Attention
python visualize_factor_attention.py \
  --artifact_dir gat_artifacts_wei2022 \
  --output_dir plots_attention_wei2022

# Step 4: åˆ†æéšå±¤å¼ç‰¹å¾µ
python analyze_contexts.py \
  --artifact_dir gat_artifacts_wei2022 \
  --output_dir plots_contexts_wei2022
```

---

## ğŸ“ è¼¸å‡ºæª”æ¡ˆ

### æ¨¡å‹æª”æ¡ˆ
- `gat_artifacts_wei2022/dmfm_wei2022_best.pt` - æœ€ä½³æ¨¡å‹
- `gat_artifacts_wei2022/train_log_wei2022.txt` - è¨“ç·´æ—¥èªŒ

### è¦–è¦ºåŒ–åœ–è¡¨
- `plots_attention_wei2022/` - Factor Attention åˆ†æ
- `plots_contexts_wei2022/` - éšå±¤å¼ç‰¹å¾µåˆ†æ

### è©•ä¼°çµæœ
- `results_dmfm_wei2022_metrics.txt` - æ¨¡å‹æŒ‡æ¨™
- `results_dmfm_wei2022_portfolio.txt` - æŠ•è³‡çµ„åˆå›æ¸¬

---

## âœ… é©—è­‰æ¸…å–®

- [x] ç§»é™¤ build_artifacts.py çš„æˆªé¢æ¨™æº–åŒ–
- [x] ç§»é™¤æ¨™ç±¤å»å‡å€¼
- [x] å¯¦ä½œå®Œæ•´çš„ DMFM_Wei2022 æ¨¡å‹
- [x] GAT ä½¿ç”¨ concat=False
- [x] ç”¢æ¥­ä¸­æ€§åŒ–ï¼šC_I = C - H_I
- [x] å…¨å¸‚å ´ä¸­æ€§åŒ–ï¼šC_U = C_I - H_Uï¼ˆè¼¸å…¥æ˜¯ C_Iï¼‰
- [x] éšå±¤å¼ç‰¹å¾µæ‹¼æ¥ï¼š[C || C_I || C_U]
- [x] Factor Attention æ¨¡çµ„
- [x] è«–æ–‡æå¤±å‡½æ•¸ï¼šd - b + IC_penalty
- [x] Factor Attention è¦–è¦ºåŒ–
- [x] éšå±¤å¼ç‰¹å¾µåˆ†æ
- [x] å®Œæ•´åŸ·è¡Œæµç¨‹è…³æœ¬
- [x] å®Œæ•´èªªæ˜æ–‡ä»¶

---

## ğŸ¯ æ¸¬è©¦å»ºè­°

1. **å…ˆä½¿ç”¨å°è³‡æ–™é›†æ¸¬è©¦**
   ```bash
   python build_artifacts.py --end_date 2020-12-31 --artifact_dir gat_artifacts_test
   python train_dmfm_wei2022.py --artifact_dir gat_artifacts_test --epochs 10
   ```

2. **æª¢æŸ¥è¦–è¦ºåŒ–è¼¸å‡º**
   ```bash
   python visualize_factor_attention.py --artifact_dir gat_artifacts_test
   ls plots_attention_wei2022/
   ```

3. **é©—è­‰éšå±¤å¼ç‰¹å¾µ**
   ```bash
   python analyze_contexts.py --artifact_dir gat_artifacts_test
   cat plots_contexts_wei2022/context_analysis_summary.txt
   ```

---

## ğŸ“š åƒè€ƒæ–‡ç»

Wei, L., Li, B., & Chen, Y. (2022). Deep Multi-Factor Model for Stock Prediction.

---

## ä½œè€…

**Lo Yi (ç¾…é ¤)**
National Yang Ming Chiao Tung University
E-mail: roy60404@gmail.com
